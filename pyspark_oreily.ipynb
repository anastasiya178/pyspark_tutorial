{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjK14+LJAGfX16AQyL8t4Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anastasiya178/pyspark_tutorial/blob/main/pyspark_oreily.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intro"
      ],
      "metadata": {
        "id": "dWZRIlUyC57p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspired by the following course:\n",
        "\n",
        "Mastering Big Data Analytics with PySpark\n",
        "https://learning-oreilly-com.hcpl.idm.oclc.org/videos/mastering-big-data/9781838640583/9781838640583-video2_1/"
      ],
      "metadata": {
        "id": "9WYsJjnLC0j9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark\n",
        "\n",
        "Spark is an in-memory tool for iteractive algorithms and interactive data mining.\n",
        "Runs on many different platforms and storage engines.\n",
        "Supports the following languages: Java, Scala, R, Python, SQL.\n",
        "Has 5 components: SQL, Streaming, ML, GraphX, Core.\n",
        "\n",
        "Spark is optimized for Big Data."
      ],
      "metadata": {
        "id": "A14dZI4aEIQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hadoop MapReduce Vs Apache Spark\n",
        "\n",
        "MapReduce must write interim results to disk.\n",
        "With Apache Spark, interim results stay in memory.\n",
        "\n",
        "Spark runs x10-x100 times faster comparing to Hadoop."
      ],
      "metadata": {
        "id": "Az3zUhbLB5Zm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "69YyvWOSCRuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cluster computing\n",
        "\n",
        "**Cluster** - a set of computers working together, each separate computer referred as **a node**.\n",
        "Master node is a main node.\n",
        "Worker nodes - all other nodes.\n",
        "\n",
        "Cluster can scale into thousand of nodes.\n",
        "\n",
        "Claster managers:\n",
        "\n",
        "*   Standalone (Spark)\n",
        "*   Kubernetes\n",
        "*   Apache Mesos\n",
        "*   Hadoop Yarn\n",
        "*   Hashicorp Nomad\n",
        "\n",
        "Driver process\n",
        "\n",
        "Spark apps use the SparkSession to acquire executors on nodes in the cluster. Executors perform tasks that are assigned to them and report back to the driver program.\n",
        "\n",
        "Use Spark Session instead Spark Context.\n",
        "The Spark Session is a part of Spark SQL component, so needs to be imported out of Spark SQL.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "crr3StBoG2Hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark dataframes"
      ],
      "metadata": {
        "id": "vusjP1wWHDom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### RDD\n",
        "Spark evolves around the concept of RDD (resilient distributed dataset) - collection of elements partitioned across the nodes of the cluster that can be operated on it in parallel.\n",
        "RDD are strongly typed, it does not infer the schema of the ingested data and requires the user to specify it.\n",
        "\n",
        "##### Dataset API\n",
        "*   A distributed collection of data that provides the benefits of RDDs with the benefits of Spark SQL's optimized execution engine\n",
        "*   Available in Java and Scala (not in Python)\n",
        "\n",
        "##### DataFrame API\n",
        "*  A distributed collection of rows (dataset) organized into named columns but with richer optimization\n",
        "*  Conceptually equivalent to a table in a relational DB\n",
        "*  A primary ML API for Spark\n",
        "*  Available in many languages including Python, R\n",
        "*  PySpark DF operations are similar to pandas DF operations\n",
        "\n",
        "##### Immutablity\n",
        "*   RDDs are immutable in nature\n",
        "*   Transformations on data in Spark result in the creation of a new Dataset, Dataframe, or RDD\n",
        "\n",
        "Example:\n",
        "df = spark.read.csv(\"some_example_data.csv\")\n",
        "\n",
        "*  *df* - dataframe\n",
        "*  *spark.read.csv* - Spark session\n",
        "*  *\"some_example_data.csv\"* - DataFrameReader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "94BVIQkoAbch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark Data Operations"
      ],
      "metadata": {
        "id": "wv9fMmALHnhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Spark is lazy.\n",
        "\n",
        "Lazy evaluation - a variety of computing techniques that delay the computation of expression until and unless the results are needed.\n",
        "\"Call by need\"\n",
        "\n",
        "Eager evaluation is the opposite of Lazy evaluation.\n",
        "\n",
        "Advantages of LE:\n",
        "*  Performance increases\n",
        "*  Reduction of memory footprint\n",
        "\n",
        "##### Data Operations\n",
        "\n",
        "*   Actions (collect, count)\n",
        "*   Transformations (agg, coalesce)\n",
        "*   Properties (columns, dtypes)\n",
        "\n",
        "\n",
        "Actions VS Transformations\n",
        "*  Data operations are responsible for changing the state of the data\n",
        "*  Transformations alter the state of the data; but they are lazy\n",
        "*  Actions also alter the state of the data; but they are blocking - Spark will perform all transformation collected up until that point\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TemIGSJJH0ts"
      }
    }
  ]
}