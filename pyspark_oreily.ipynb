{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAQqGBSZ7e2bJCFpyX2PBr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anastasiya178/pyspark_tutorial/blob/main/pyspark_oreily.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intro"
      ],
      "metadata": {
        "id": "dWZRIlUyC57p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspired by the following course:\n",
        "\n",
        "Mastering Big Data Analytics with PySpark\n",
        "https://learning-oreilly-com.hcpl.idm.oclc.org/videos/mastering-big-data/9781838640583/9781838640583-video2_1/"
      ],
      "metadata": {
        "id": "9WYsJjnLC0j9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark\n",
        "\n",
        "Spark is an in-memory tool for iteractive algorithms and interactive data mining.\n",
        "Runs on many different platforms and storage engines.\n",
        "Supports the following languages: Java, Scala, R, Python, SQL.\n",
        "Has 5 components: SQL, Streaming, ML, GraphX, Core.\n",
        "\n",
        "Spark is optimized for Big Data."
      ],
      "metadata": {
        "id": "A14dZI4aEIQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hadoop MapReduce Vs Apache Spark\n",
        "\n",
        "MapReduce must write interim results to disk.\n",
        "With Apache Spark, interim results stay in memory.\n",
        "\n",
        "Spark runs x10-x100 times faster comparing to Hadoop."
      ],
      "metadata": {
        "id": "Az3zUhbLB5Zm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "69YyvWOSCRuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cluster computing\n",
        "\n",
        "**Cluster** - a set of computers working together, each separate computer referred as **a node**.\n",
        "Master node is a main node.\n",
        "Worker nodes - all other nodes.\n",
        "\n",
        "Cluster can scale into thousand of nodes.\n",
        "\n",
        "Claster managers:\n",
        "\n",
        "*   Standalone (Spark)\n",
        "*   Kubernetes\n",
        "*   Apache Mesos\n",
        "*   Hadoop Yarn\n",
        "*   Hashicorp Nomad\n",
        "\n",
        "Driver process\n",
        "\n",
        "Spark apps use the SparkSession to acquire executors on nodes in the cluster. Executors perform tasks that are assigned to them and report back to the driver program.\n",
        "\n",
        "Use Spark Session instead Spark Context.\n",
        "The Spark Session is a part of Spark SQL component, so needs to be imported out of Spark SQL.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "crr3StBoG2Hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vusjP1wWHDom"
      }
    }
  ]
}